<html>
<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Can Qin's Homepage</title>
  <meta content="Can Qin, github.com/ChinTsan01" name="keywords" />
  <style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
a {
  color: #1772d0;
  text-decoration:none;
}
a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}
a.paper {
  font-weight: bold;
  font-size: 14pt;
}
b.paper {
  font-weight: bold;
  font-size: 14pt;
}
* {
  margin: 0pt;
  padding: 0pt;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1000px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}
h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 17pt;
  font-weight: 700;
}
h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18px;
  font-weight: 700;
}
strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight:bold;
}
ul { 
  list-style: circle;
}
img {
  border: none;
}
li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight: bold;
  color: #FF0000;
}
em, i {
  font-style:italic;
}
div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}
div.spanner {
  clear: both;
}
div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
div.paper div {
  padding-left: 10px;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}    

span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}
pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}
div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-45959174-3', 'kailigo.github.io');
  ga('send', 'pageview');
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');
</script>

  
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Can Qin" style="float: left; padding-left: .5em; height: 140px;" src="qincan01_4.JPG" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Can Qin 秦灿</span><br />
<span><strong>Ph.D. Candidate</strong></span><br />
<!--<span><a href='http://cvrs.whu.edu.cn/'>Computer Vision & Remote Sensing (CVRS) Lab </a> <br /> </q>-->
<span>Electrical and Computer Engineering Department, Northeastern University, Boston, MA, USA.</span><br />
<span><strong>Office</strong>: Richard Hall, 360 Huntington Ave, Boston, MA 02115</span><br />
<span><strong>Email </strong>: <STRIKE> qin.ca [at] husky.neu.edu </STRIKE>  &nbsp qin.ca [at] northeastern.edu  &nbsp &nbsp  
<!-- <strong><a href='https://chintsan01.github.io/CV.pdf'>CV</a></strong>  &nbsp &nbsp -->
<strong><a href='https://github.com/canqin001'>Github</a></strong>  &nbsp &nbsp
<strong><a href='https://www.linkedin.com/in/can-qin-5b7b33168/'>LinkedIn</a></strong>  &nbsp &nbsp
<strong><a href='https://scholar.google.com/citations?user=QCik-YcAAAAJ&hl=en&authuser=1'>Google Scholar</a></strong></span><br/>
<!-- <span><strong> <a href='https://github.com/kailigo'>Github</a> </strong></span> <br /> -->
<!-- <span><strong> <a href='https://scholar.google.com/citations?hl=en&user=YsROc4UAAAAJ'>Google Scholar</a> </strong></span> <br /> -->
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->



<div style="clear: both;">
<div class="section">
<h2>About Me</h2>
<div class="paper">
I am a fifth-year Ph.D. candidate in the <a href='https://web.northeastern.edu/smilelab/'>Smile Lab</a> of <a href='http://www.ece.neu.edu/'>Department of Electrical and Computer Engineering</a>, <a href='http://www.northeastern.edu/'>Northeastern University (NEU)</a> under the supervision of <a href='http://www1.ece.neu.edu/~yunfu/'>Prof. Yun Raymond Fu</a>. 
I received my B.E. degree from the School of Microelectronics, <a href='https://en.xidian.edu.cn'>Xidian University (XDU)</a>, China, in 2018. My research interests broadly include the theories and applications in machine learning, computer vision and data mining, with the high focus on <strong>data efficiency</strong> and <strong>model efficiency</strong>. 




</div>
<div class="paper">
  <strong> I am actively looking for opportunities in the industry, and please feel free to  <a href='Can Qin: qin.ca@northeastern.edu'>contact me</a> if interested. </strong>

</div>
</div>
</div>

<div style="clear: both;">
  <div class="section">
    <h2>Research Interests</h2>
    <div class="paper">
      <ul>
    <li> Transfer Learning, Knowledge Distillation and Domain Adaptation.</li>
    <li> Self-supervised/Semi-supervised/Few-shot/Zero-shot/Incremental Learning.</li>
    <li> Efficient Deep Learning for Image Classification, Segmentation and 3D Vision.</li>
    </div>
  </div>
  </div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <div style="overflow-y: scroll; height:400px; width:970px">
    <ul> 

      <!-- <b><font face="Arial" size="4">News</font></b>
      <div style="overflow-y: scroll; height:265px; width:1100px">
      <table border="1" style="border-width: 0px;" width="1050">
      <tbody>
      <tr>
      <td style="border-style: none; border-width: medium;">
      
      <ul class="STYLE223"> --> 

  <li> 2022.09: One paper is accepted by <strong> ICDM 2022</strong> and one paper is accepted by <strong>IEEE Transactions on Image Processing (TIP)</strong>. </li>
  <li> 2022.08: I have been invited as a PC member for <strong> AAAI 2023 </strong> and a reviewer for <strong> ICLR 2023 </strong>. </li>
  <li> 2022.08: We have two papers accepted by <strong> CIKM 2022</strong>. </li>
  <li> 2022.05: We have a paper accepted by <strong> KDD 2022</strong>. Thanks to all the mentors from Adobe Research. </li>
  <li> 2022.05: I start my summer internship at <strong> <a href='https://www.salesforceairesearch.com' > Salesforce Research</a>  </strong> at Palo Alto, CA.</li>  
  <li> 2022.04: We have two papers accepted by <strong> IJCAI 2022</strong>. </li>
  <li> 2022.02: The <a href='https://github.com/ma-xu/pointMLP-pytorch' >official code</a> of PointMLP is released. I have been invited as the reviewer for TPAMI and TMLR.  </li>
  <li> 2022.01: We have two papers accepted by <strong> ICLR 2022</strong>. Congrats to Xu and Yulun. </li>
  <li> 2022.01: I have been invited as a reviewer for <strong> ICML 2022 </strong>. </li>
  <li> 2021.12: The manuscript of our new paper - <a href='https://arxiv.org/pdf/2112.06161.pdf' > Semi-supervised Domain Adaptive Structure Learning </a> has been uploaded.</li>
  <li> 2021.11: I have been invited as a reviewer for <strong> CVPR 2022 </strong> and <strong> TNNLS </strong>. </li>
  <li> 2021.11: Our paper about AI + Science (i.e., Topology Optimization) has been accepted by <strong> <a href='https://www.nature.com/ncomms/' > Nature Communications </a> </strong>.</li>
  <li> 2021.10: Our  <a href='https://arxiv.org/abs/2106.11514' > new paper - AdaMomentum </a> for a general-purpose deep learning optimizer has been uploaded.</li>
  <li> 2021.09: We have two papers accepted by <strong> NeurIPS 2021</strong>, with one Poster and one Spotlight respectively. </li>
  <li> 2021.09: We have a paper accepted by <strong> IEEE Transactions on Image Processing (TIP) </strong>. </li>
  <li> 2021.08: We have a paper accepted by <strong> International Conference on Data Mining (ICDM) 2021 </strong>. </li>
  <li> 2021.07: We have a paper accepted by <strong> International Conference on Computer Vision (ICCV) 2021 </strong>. </li>
  <li> 2021.07: I have been invited as a  <strong> program committee (PC) member </strong> for <strong> IJCAI 2022 </strong>. </li>
  <li> 2021.07: Our paper is accepted by <strong> ACM Multimedia (MM) 2021 </strong>. </li>
  <li> 2021.06: I have been invited as a  <strong> reviewer </strong> for <strong> ICLR 2022 </strong>. </li>
  <li> 2021.04: I have received the  <strong> SIAM Student Travel Award </strong> to support the paper presentation at <strong>  <a href='https://www.siam.org/conferences/cm/conference/sdm21' >SDM 2021</a></strong>. </li>          
  <li> 2021.04: I have been invited as a  <strong> reviewer </strong> for <strong>  <a href='https://nips.cc' >NeurIPS 2021</a></strong>. </li>
  <li> 2021.03: Our <a href='https://arxiv.org/abs/2103.09118' > journal extension </a>  of <a href='https://arxiv.org/abs/2002.06483' > the unbiased Face Recognition paper </a>   is uploaded to the arXiv. </li>
  <li> 2021.03: Our new  <a href='https://arxiv.org/abs/2103.06460' >Neural Pruning survey paper</a>  is uploaded to the arXiv. </li>
  <li> 2021.03: I have been invited as a  <strong> reviewer </strong> for <strong> IEEE Robotics and Automation Letters (RA-L) </strong>. </li>
  <li> 2021.02: I have been invited as a  <strong> reviewer </strong> for <strong>  <a href='http://iccv2021.thecvf.com' >ICCV 2021</a></strong>. </li>
  <li> 2021.01: I have been invited as a  <strong> reviewer </strong> for <strong> IEEE Transactions on Image Processing (TIP) </strong>. </li>
  <li> 2021.01: Our <a href='https://arxiv.org/abs/2012.09243' >Neural Pruning paper</a>  is accepted by <strong> ICLR 2021 </strong> as <strong> Poster. </strong> </li>
  <li> 2020.12: Our <a href='https://arxiv.org/abs/2002.02545' >Semi-supervised DA paper</a>  is accepted as a regular paper by <strong> SDM 2021 </strong>. </li>
  <li> 2020.12: Our new <a href='https://arxiv.org/abs/2012.04111' >Face Synthesis paper</a>  is uploaded to the arXiv. </li>
  <li> 2020.12: I will start my 2021 summer intern at <strong> <a href='https://research.adobe.com' >Adobe Research</a></strong> remotely.</li>  
  <li> 2020.12: I have been invited as a  <strong> reviewer </strong> for <strong> CVPR 2021 </strong>. </li>
  <li> 2020.11: I have been promoted as a  <strong> senior program committee (SPC) member </strong> for <strong> IJCAI 2021 </strong>. </li>
  <li> 2020.09: I have been invited as a  <strong> program committee (PC) member </strong> for <strong> AAAI 2021 </strong>. </li>
  <li> 2020.08: I have been invited as a  <strong> program committee (PC) member </strong> for <strong> IJCAI 2021 </strong>. </li>
  <li> 2020.07: Our paper is accepted by <strong> ECCV 2020 </strong> as <strong> Poster. </strong></li>
  <li> 2020.05: Our paper is accepted by <strong> <a href='https://fadetrcv.github.io'>CVPR Workshop on Fair, Data Efficient and Trusted Computer Vision, 2020. </a> </strong></li>
  <li> 2019.12: I have been invited as a  <strong> program committee (PC) member </strong> for <strong> IJCAI-PRICAI 2020 </strong>. </li>
  <li> 2019.11: Our paper is accepted by <strong> AAAI 2020 </strong> as <strong> Poster. </strong></li>
  <li> 2019.10: Our paper is awarded as <strong> <a href='https://chintsan01.github.io/pic/ICCW_Award.jpeg'>the Best Paper</a> </strong> of <strong><a href='https://www.forlq.org'>ICCV Workshop on RLQ, 2019.</a> </strong></li>
  <li> 2019.09: Our paper is accepted by <strong> NeurIPS 2019 </strong> as <strong> Poster. </strong></li>
  <li> 2019.08: Our paper is accepted by <strong> ICCVW on RLQ, 2019 </strong> as <strong> Oral. </strong></li>
  <li> 2019.06: Start my internship at <strong> Adobe </strong> in San Jose.</li>  
  <li> 2018.09: Begin my journey in <strong> Smile Lab, Northeastern University </strong> at Boston.</li>   
    </ul>
  </div>
  </div>
</div>
</div>

<!-- </ul>
</td>
</tr>
</tbody>
</table>
</div> -->





<h2>Experiences</h2>
<div style="clear: both;">
  <div class="paper">
  
  

<table border="0" cellpadding="0" cellspacing="0" bordercolor="#FF0000" >
<tbody >

  <td width="800">
    <span style="float:left">
  <font face="Arial" size="2" style="line-height:1.5;">
  <p>&nbsp;&nbsp;&nbsp; <a href='https://web.northeastern.edu/smilelab/'>SMILE Lab</a >, Northeastern University, Boston, USA</p >
  <p>&nbsp;&nbsp;&nbsp; Research Assistant, &nbsp; Sep. 2018 ~ Now</p >
  <p>&nbsp;&nbsp;&nbsp; Supervisor: Prof. <a href="http://www1.ece.neu.edu/~yunfu/">Yun Raymond Fu</a ></p >
  </font>
</span>
  </td>



  <td width="150" height="93">
    <span style="float:right">
    <p><img src="./pic/NEU-logo.png" height="78"> </p>
  </span>
  </td>


</tbody>

<tbody >

  <td width="800">
    <span style="float:left">
  <font face="Arial" size="2" style="line-height:1.5;" >
  <p>&nbsp;&nbsp;&nbsp; Salesforce Research, Palo Alto, USA</p >
  <p>&nbsp;&nbsp;&nbsp; Research Intern, &nbsp; May 2022 ~ Dec. 2022</p >
  </font>
</span>
  </td>

  <td width="150" height="85">
    <span style="float:right">
    <p><img src="./pic/SF-logo.svg.png" height="55"> </p>
  </span>
  </td>

</tbody>


<tbody >

  <td width="800">
    <span style="float:left">
  <font face="Arial" size="2" style="line-height:1.5;" >
  <p>&nbsp;&nbsp;&nbsp; Adobe Research, San Jose, USA</p >
  <p>&nbsp;&nbsp;&nbsp; Research Intern, &nbsp; June 2021 ~ Sep. 2021</p >
  <p>&nbsp;&nbsp;&nbsp; Mentors: Dr. Sungchul Kim, Dr. Handong Zhao, Dr. Tong Yu and Dr. Ryan Rossi</p >
  </font>
</span>
  </td>

  <td width="150" height="85">
    <span style="float:right">
    <p><img src="./pic/Adobe-logo.png" height="64"> </p>
  </span>
  </td>

</tbody>


<tbody >

  <td width="800">
    <span style="float:left">
  <font face="Arial" size="2" style="line-height:1.5;" >
  <p>&nbsp;&nbsp;&nbsp; Adobe, San Jose, USA</p >
  <p>&nbsp;&nbsp;&nbsp; Data Science Intern, &nbsp; June 2019 ~ Aug. 2019</p >
  <p>&nbsp;&nbsp;&nbsp; Mentors: Dr. Jie Zhang, Dr. Yiwen Sun and Dr. Bo Peng</p >
  </font>
</span>
  </td>

  <td width="150" height="85">
    <span style="float:right">
    <p><img src="./pic/Adobe-logo.png" height="64"> </p>
  </span>
  </td>

</tbody>




  <tbody >
    <td width="800">
      <span style="float:left">
    <font face="Arial" size="2" style="line-height:1.5;" >
    <p>&nbsp;&nbsp;&nbsp; OMEGA Lab, Xidian University, Xi'an, China</p >
    <p>&nbsp;&nbsp;&nbsp; Visiting Research Assistant, &nbsp; Sep. 2017 ~ June 2018</p >
    <p>&nbsp;&nbsp;&nbsp; Mentors: Prof. Maoguo Gong and Prof. Yue Wu</a ></p >
    </font>
  </span>
    </td>
    <td width="150" height="73">
      <span style="float:right">
      <p><img src="./pic/Xidian-logo.png" height="77"> </p>
    </span>
    </td>
  </tbody>


</table>
</div>
</div>
  




 

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Selected Conference Papers</h2>

<div class="paper" id="ICDM-VAD">
  <div> <strong>Making Reconstruction-based Method Great Again for Video Anomaly Detection</strong><br>
    Yizhou Wang, <strong><u>Can Qin</u></strong>, Yue Bai, Yi Xu, Xu Ma, Yun Fu <br>
    IEEE International Conference on Data Mining (<strong><i>ICDM</i></strong>), 2022. <br>
  <a href='' >[Paper]</a> 
  <a href=''' >[Code]</a>
  </div>
  <div class="spanner"></div>
  </div>

  <div class="paper" id="CIKM-NoisyDA">
    <div> <strong>Robust Semi-supervised Domain Adaptation against Noisy Labels</strong><br>
      <strong><u>Can Qin</u></strong>, Yizhou Wang,  Yun Fu <br>
      ACM International Conference on Information and Knowledge Management (<strong><i>CIKM</i></strong>), 2022. <br>
    <a href='' >[Paper]</a> 
    <a href='' >[Code]</a>
    </div>
    <div class="spanner"></div>
    </div>

    <div class="paper" id="CIKM-SAD">
      <div> <strong>Self-supervision Meets Adversarial Perturbation: A Novel Framework for Anomaly Detection </strong><br>
        Yizhou Wang, <strong><u>Can Qin</u></strong>, Rongzhe Wei, Yi Xu, Yue Bai, Yun Fu <br>
        ACM International Conference on Information and Knowledge Management (<strong><i>CIKM</i></strong>), 2022. <br>
      <a href='' >[Paper]</a> 
      <a href='' >[Code]</a>
      </div>
      <div class="spanner"></div>
      </div>

<div class="paper" id="KDD-TabularKG">
  <div> <strong>External Knowledge Infusion for Tabular Pre-training Models with Dual-adapters</strong><br>
    <strong><u>Can Qin</u></strong>, Sungchul Kim, Handong Zhao, Tong Yu, Ryan Rossi,  Yun Fu <br>
    ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (<strong><i>KDD</i></strong>), 2022. <br>
  <a href='https://dl.acm.org/doi/abs/10.1145/3534678.3539403' >[Paper]</a> 
  <a href='' >[Code]</a>
  </div>
  <div class="spanner"></div>
  </div>

<!-- <div class="paper" id="IJCAI-MemREIN">
  <div> <strong>MemREIN: Rein the Domain Shift for Cross-Domain Few-Shot Learning</strong><br>
   Yi Xu, Lichen Wang, Yizhou Wang, <strong><u>Can Qin</u></strong>, Yulun Zhang,  Yun Fu <br>
    International Joint Conference on Artificial Intelligence (<strong><i>IJCAI</i></strong>), 2022. <br>
  <a href='' >[Paper]</a> 
  <a href=''' >[Code]</a>
  </div>
  <div class="spanner"></div>
  </div> -->

<div class="paper" id="IJCAI-Survey-Pruning">
  <div> <strong>Emerging Paradigms of Neural Network Pruning</strong><br>
    Huan Wang, <strong><u>Can Qin</u></strong>, Yue Bai, Yulun Zhang,  Yun Fu <br>
    International Joint Conference on Artificial Intelligence (<strong><i>IJCAI</i></strong>), 2022. <br>
  <a href='https://arxiv.org/abs/2103.06460' >[arXiv]</a> 
  <a href='https://github.com/MingSun-Tse/Smile-Pruning' >[Code]</a>
  </div>
  <div class="spanner"></div>
  </div>

<div class="paper" id="ICLR-2">
  <div> <strong>Rethinking Network Design and Local Geometry in Point Cloud: A Simple Residual MLP Framework</strong><br>
    Xu Ma, <strong><u>Can Qin</u></strong>, Haoxuan You, Haoxi Ran and Yun Fu <br>
    International Conference on Learning Representations (<strong><i>ICLR</i></strong>), 2022.<br>
  <a href='https://openreview.net/pdf?id=3Pbra-_u76D'>[Paper]</a>
  <a href='https://github.com/ma-xu/pointMLP-pytorch' >[Code]</a>
</div>
  <div class="spanner"></div>
  </div>

<!-- <div class="paper" id="ICLR-1">
  <div> <strong>Learning Efficient Image Super-Resolution Networks via Structure-Regularized Pruning</strong><br>
    Yulun Zhang*, Huan Wang*, <strong><u>Can Qin</u></strong> and Yun Fu <br>
    International Conference on Learning Representations (<strong><i>ICLR</i></strong>), 2022.<br>
  <a href='https://openreview.net/pdf?id=AjGC97Aofee'>[Paper]</a>
</div>
  <div class="spanner"></div>
  </div> -->


<div class="paper" id="NIPS21-1">
  <div> <strong>Slow Learning and Fast Inference: Efficient Graph Similarity Computation via Knowledge Distillation</strong><br>
    <strong><u>Can Qin</u></strong>, Handong Zhao, Lichen Wang, Huan Wang, Yulun Zhang and Yun Fu <br>
    Advances in Neural Information Processing Systems (<strong><i>NeurIPS</i></strong>), 2021.<br>
  <a href='https://papers.nips.cc/paper/2021/file/75fc093c0ee742f6dddaa13fff98f104-Paper.pdf' >[Paper]</a>
  <a href='https://github.com/canqin001/Efficient_Graph_Similarity_Computation'>[Code]</a>
  </div>
  <div class="spanner"></div>
  </div>

  <!-- <div class="paper" id="NIPS21-2">
    <div> <strong>Aligned Structured Sparsity Learning for Efficient Image Super-Resolution</strong><br>
      Yulun Zhang*, Huan Wang*, <strong><u>Can Qin</u></strong> and Yun Fu <br>
      Advances in Neural Information Processing Systems (<strong><i>NeurIPS</i></strong>), 2021. (<strong><u>Spotlight, 3%</u></strong>)<br>
    <a href='https://papers.nips.cc/paper/2021/file/15de21c670ae7c3f6f3f1f37029303c9-Paper.pdf'>[Paper]</a>
    <a href='https://github.com/MingSun-Tse/ASSL'>[Code]</a>  
  </div>
    <div class="spanner"></div>
    </div> -->


<!-- <div class="paper" id="ContextSR">
  <div> <strong>Context Reasoning Attention Network for Image Super-Resolution</strong><br>
    Yulun Zhang, Donglai Wei, <strong><u>Can Qin</u></strong>, Huan Wang, Hanspeter Pfister, Yun Fu <br>
    International Conference on Computer Vision (<strong><i>ICCV</i></strong>), 2021.<br>
  <a href='  https://vcg.seas.harvard.edu/publications/context-reasoning-attention-network-for-image-super-resolution'>[Paper]</a>
  </div>
  <div class="spanner"></div>
  </div> -->


  
<div class="paper" id="NeuralPruning">
  <div> <strong>Neural Pruning via Growing Regularization</strong><br>
    Huan Wang, <strong><u>Can Qin</u></strong>, Yulun Zhang, Yun Fu <br>
    International Conference on Learning Representations (<strong><i>ICLR</i></strong>), 2021.<br>
<a href='https://openreview.net/pdf?id=o966_Is_nPA' >[Paper]</a>
<a href='https://arxiv.org/abs/2012.09243' >[arXiv]</a>
<a href='https://github.com/MingSun-Tse/Regularization-Pruning' >[Code]</a>
<a href='https://chintsan01.github.io/bib/NeuralPruningICLR20.bib' >[BibTex]</a>
</div>
<div class="spanner"></div>
</div>

<div class="paper" id="UODA"><!-- <img class="paper" src="./pic/UODA.png" title="Opposite Structure Learning for Semi-supervised Domain Adaptation">-->
  <div> <strong>Contradictory Structure Learning for Semi-supervised Domain Adaptation</strong><br>
    <strong><u>Can Qin</u></strong>, Lichen Wang, Qianqian Ma, Yu Yin, Huan Wang, Yun Fu <br>
    SIAM International Conference on Data Mining (<strong><i>SDM</i></strong>), 2021.<br>
  <a href='https://epubs.siam.org/doi/pdf/10.1137/1.9781611976700.65'>[Paper]</a>
  <a href='https://arxiv.org/abs/2002.02545' >[arXiv]</a>
  <a href='https://github.com/canqin001/Contradictory-Structure-Learning-for-Semi-supervised-Domain-Adaptation' >[Code]</a>
  <a href='https://chintsan01.github.io/bib/UODA.bib' >[BibTex]</a>
  </div>
  <div class="spanner"></div>
  </div>

<!-- <div class="paper" id="ECCV20">
<div> <strong>Generative View-Correlation Adaptation for Semi-Supervised Multi-View Learning</strong><br>
  Yunyu Liu, Lichen Wang, Yue Bai, <strong><u>Can Qin</u></strong>, Zhengming Ding, Yun Fu <br>
  European Conference on Computer Vision (<strong><i>ECCV</i></strong>), 2020.<br>
  <a href='http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590307.pdf' >[Paper]</a>
 <a href='https://github.com/wenwen0319/GVCA' >[Code]</a>
 <a href='https://chintsan01.github.io/bib/ECCV20.bib' >[BibTex]</a>
</div>
<div class="spanner"></div>
</div>  -->
<!-- <img class="paper" src="./pic/ECCV20.png" title="Generative View-Correlation Adaptation for Semi-Supervised Multi-View Learning">-->

  
 <!-- <div class="paper" id="AAAI20">
<div> <strong>Dual Relation Semi-supervised Multi-label Learning</strong><br>
  Lichen Wang, Yunyu Liu, <strong><u>Can Qin</u></strong>, Gan Sun, Yun Fu <br>
  AAAI Conference on Artificial Intelligence (<strong><i>AAAI</i></strong>), 2020.<br>
<a href='https://github.com/wanglichenxj/Dual-Relation-Semi-supervised-Multi-label-Learning/blob/master/presentation/AAAI20_MultiLabel.pdf' >[Paper]</a>
<a href='https://github.com/wanglichenxj/Dual-Relation-Semi-supervised-Multi-label-Learning' >[Code]</a>
<a href='https://chintsan01.github.io/bib/AAAI20.bib' >[BibTex]</a>
</div>
<div class="spanner"></div>
</div> -->
<!-- <img class="paper" src="./pic/AAAI20.png" title="Dual Relation Semi-supervised Multi-label Learning">-->


<div class="paper" id="PointDAN"><!-- <img class="paper" src="./pic/PointDAN.png" title="PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation">-->
<div> <strong>PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation</strong><br>
  <strong><u>Can Qin*</u></strong>, Haoxuan You*, Lichen Wang, C.-C. Jay Kuo, Yun Fu. (* equal contribution) <br>
  Advances in Neural Information Processing Systems (<strong><i>NeurIPS</i></strong>), 2019.<br>
<a href='http://papers.nips.cc/paper/8940-pointdan-a-multi-scale-3d-domain-adaption-network-for-point-cloud-representation.pdf'>[Paper]</a>
<a href='https://github.com/canqin001/PointDAN'>[Code]</a>
<a href='https://chintsan01.github.io/bib/PointDAN.bib'>[BibTex]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="GICT"><!-- <img class="paper" src="./pic/GICT.png" title="Generatively Inferential Co-Training for Unsupervised Domain Adaptation">-->
<div> <strong>Generatively Inferential Co-Training for Unsupervised Domain Adaptation</strong><br>
<strong><u>Can Qin</u></strong>, Lichen Wang, Yulun Zhang, Yun Fu.<br>
ICCV Workshop on Real-World Recognition from Low-Quality Images and Videos, 2019. (<strong><u><a href='https://chintsan01.github.io/pic/ICCW_Award.jpeg'>Best Paper Award</a> </u></strong>)<br>
<a href='http://openaccess.thecvf.com/content_ICCVW_2019/papers/RLQ/Qin_Generatively_Inferential_Co-Training_for_Unsupervised_Domain_Adaptation_ICCVW_2019_paper.pdf'>[Paper]</a>
<a href='https://chintsan01.github.io/bib/GICT.bib'>[BibTex]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="more"><!-- <img class="paper" src="./pic/GICT.png" title="Generatively Inferential Co-Training for Unsupervised Domain Adaptation">-->
  <div><a href='https://scholar.google.com/citations?user=QCik-YcAAAAJ&hl=en&authuser=1'><u>more papers</u></a></div>
  <div class="spanner"></div>
</div>

</div>
</div>







<!-- <div class="paper" id="AAAIW18">
<div> <strong>Efficient Scene Labeling via Sparse Annotations</strong><br>
  <strong><u>Can Qin</u></strong>, Maoguo Gong, Yue Wu, Dayong Tian, Puzhao Zhang.<br>
Smart IoT Workshop at the AAAI Conference on Artificial Intelligence, 2018.<br>
<a href='https://www.aaai.org/ocs/index.php/WS/AAAIW18/paper/viewFile/17020/15569'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>


 <div class="paper" id="MFLR">
  <div> <strong>A Multi-objective Framework for Location Recommendation Based on User Preference</strong><br>
    Shanfeng Wang, Maoguo Gong, <strong><u>Can Qin</u></strong>, Junwei Yang<br>
    IEEE Conference on Computational Intelligence and Security (<strong>CIS</strong>), 2017<br>
  <a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288439'>[Paper]</a>
  </div>
  <div class="spanner"></div>
  </div>

 <div class="paper" id="LPMF">
<div> <strong>Local Probabilistic Matrix Factorization for Personal Recommendation</strong><br>
  Wenping Ma, Yue Wu, Maoguo Gong, <strong><u>Can Qin</u></strong>, Shanfeng Wang.<br>
  IEEE Conference on Computational Intelligence and Security (<strong>CIS</strong>), 2017<br>
<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8288451'>[Paper]</a>
</div>
<div class="spanner"></div>
</div>  -->



<div style="clear: both;">
  <div class="section">
  <h2 id="journalpapers">Journal Papers</h2>

  <div class="paper" id="ASDA">
    <div> <strong>Semi-supervised Domain Adaptive Structure Learning</strong><br>
      <strong><u>Can Qin</u></strong>, Lichen Wang, Qianqian Ma, Yu Yin, Huan Wang, Yun Fu <br>
      IEEE Transactions on Image Processing (<strong><i>TIP</i></strong>), 2022. <br>
    <a href='' >[Paper]</a> 
    <a href='https://arxiv.org/abs/2112.06161' >[arXiv]</a>
    <a href='https://github.com/canqin001/ASDA'>[Code]</a>  
    </div>
    <div class="spanner"></div>
    </div>

  <div class="paper" id="NC22">
    <div> <strong>Self-Directed Online Machine Learning for Topology Optimization</strong><br>
      Changyu Deng, Yizhou Wang, <strong><u>Can Qin</u></strong>, Yun Fu, Wei Lu <br>
      Nature Communications (<strong><i>Nature Comm</i></strong>), 2022. <br>
    <a href='https://www.nature.com/articles/s41467-021-27713-7'>[Paper]</a>
    </div>
    <div class="spanner"></div>
    </div>

  <!-- <div class="paper" id="TIP21">
    <div> <strong>Semi-supervised Dual Relation Learning for Multi-label Classification</strong><br>
      Lichen Wang, Yunyu Liu, Hang Di, <strong><u>Can Qin</u></strong>, Gan Sun, Yun Fu <br>
      IEEE Transactions on Image Processing (<strong><i>TIP</i></strong>), 2021. <br>
    <a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600608'>[Paper]</a>
    </div>
    <div class="spanner"></div>
    </div> -->
  
  </div>
</div>
</div>


<div style="clear: both;">
  <div class="section">
  <h2 id="confpapers">Pre-print Papers</h2>

  

  <div class="paper" id="AdaMomentum">
    <div> <strong>Adapting Stepsizes by Momentumized Gradients Improves Optimization and Generalization</strong><br>
      Yizhou Wang, Yue Kang, <strong><u>Can Qin</u></strong>, Yi Xu, Huan Wang, Yulun Zhan, Yun Fu <br>
      arXiv:2106.11514, 2021.<br>
    <a href='https://arxiv.org/abs/2106.11514' >[arXiv]</a>
    </div>
    <div class="spanner"></div>
    </div>

  <div class="paper" id="FaceBiasJournal">
    <div> <strong>Balancing Biases and Preserving Privacy on Balanced Faces in the Wild</strong><br>
      Joseph P Robinson,  <strong><u>Can Qin</u></strong>, Yann Henon, Samson Timoner,  Yun Fu <br>
      arXiv:2103.09118, 2021.<br>
    <a href='https://arxiv.org/abs/2103.09118' >[arXiv]</a>
    </div>
    <div class="spanner"></div>
    </div>


<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Awards</h2>
<div class="paper">
    <ul> 
        <li>SIAM Student Travel Award, 2021</li>
        <li>Best Paper Award of ICCV Workshop on RLQ, 2019</li>
        <li>The Star of Graduates in Class 2018 (Highest Honor in XDU), 2018</li>
        <li>The First Prize Scholarship in XDU, 2016, 2017</li>
        <li>Meritorious Winner of the Interdisciplinary Contest in Modeling, 2016 </li>
        <li>Outstanding Student Leader in XDU, 2015</li>  
    <!-- <li> Best Undergraduate Thesis, School of Remote Sensing and Information Engineering, Wuhan University, 2014. </li> -->
    <!-- <li> Excellent Undergraduate Students, Wuhan University, 2012. </li> -->
    </ul>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professonal Activities</h2>
<div class="paper">
<ul>
<p><font size="5">
    <li>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Reviewer </li>
    <li>Transactions on Machine Learning Research (TMLR), Reviewer </li>
    <li>ACM Transactions on Knowledge Discovery from Data (TKDD), Reviewer</li>
    <li>IEEE Robotics and Automation Letters (RA-L), Reviewer</li>
    <li>IEEE Transactions on Image Processing (TIP), Reviewer</li>
    <li>IEEE Transactions on Neural Networks and Learning Systems (TNNLS), Reviewer</li>
    <li>IEEE Computational Intelligence Magazine, External Reviewer</li>
    <li>International Joint Conference on Artificial Intelligence (IJCAI), SPC 21, PC 20, 22 </li>
    <li>AAAI Conference on Artificial Intelligence (AAAI), PC 21, 22 </li>
    <li>Conference on Neural Information Processing Systems (NeurIPS), Reviewer 21, 22, 23</li>
    <li>International Conference on Learning Representations (ICLR), Reviewer 22, 23</li>
    <li>International Conference on Machine Learning (ICML), Reviewer 22</li>
    <li>The Conference on Computer Vision and Pattern Recognition (CVPR), PC 21, Reviewer 22</li>
    <li>International Conference on Computer Vision (ICCV), PC 21</li>
    <li>European Conference on Computer Vision (ECCV),  Reviewer 22</li>
    <li>IEEE International Conference on Automatic Face and Gesture Recognition (FG), Volunteer 18, Reviewer 21</li>
    <li><a href='https://medium.com/to-recognize-families-in-the-wild-a-machine-vision/rfiw2021-7ceb357a39a6'>5th Recognizing Families In the Wild (RFIW)</a>, <a href='https://arxiv.org/pdf/2111.00598.pdf' >[White Paper Link]</a>, with FG21, Program Co-Chair </li>
    
    
  <!-- <li> Reviewer for CVPR 2019, 2020, ICCV 2019, AAAI 2019, 2020, TIP, TNNLS, TMI, TMM </li>   -->     
  <!-- <li> External Reviewer for CVPR, ICCV, ECCV, AAAI, IJCAI, AAAI (2017). </li> -->
    <!-- <li> Guest reviewer for AAAI 2017. </li>       -->
  <!-- <li> IEEE student member, AAAI student member. </li>    -->
</font></p>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
  <div class="section">
  <h2 id="talks">Talks</h2>
  <div class="paper">
  <ul>
  <p><font size="5">
    
    <li><a href='https://db.khoury.northeastern.edu/activities/'>[09/30/2021] Data Lab @ NEU</a></li>
    <!-- <li> Reviewer for CVPR 2019, 2020, ICCV 2019, AAAI 2019, 2020, TIP, TNNLS, TMI, TMM </li>   -->     
    <!-- <li> External Reviewer for CVPR, ICCV, ECCV, AAAI, IJCAI, AAAI (2017). </li> -->
      <!-- <li> Guest reviewer for AAAI 2017. </li>       -->
    <!-- <li> IEEE student member, AAAI student member. </li>    -->
  </font></p>
  </ul>
  </div>
  </div>
  </div>

<div style="clear: both;">
  <div class="section">
  <h2 id="demos">Project Demos</h2>

  <div class="paper" id="OCRDemo"><img style="float: left; height: 120px;"  src="pic/ocr-demo.gif" title="OCR">
    <!-- <div> <strong> Optical character recognition (OCR) for Banner Images </strong><br> -->
      <div style="padding-left: 15em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 15pt;"> Project: Optical Character Recognition (OCR) for Banner Images</span><br />
        <span><u>Tools:</u> Tesseract OCR, OpenCV</span><br />
        <span><u>Methods:</u> EAST (Text Localization), LSTM (Text Recognition) </span><br />
        <span><u>Pipeline:</u> RGB -> Gray -> Gaussian Filtering -> Binarization -> OCR </span><br />
        <span><u>Full Demo:</u><a href='https://github.com/ChinTsan01/ChinTsan01.github.io/blob/master/pic/ocr-demo.gif'> Link</span><br />
        <div></a></div>
      </div>
    <div class="spanner"></div>
    </div>

  </div>
</div>
</div>

<div style="clear: both;">
  <div class="section">
  <h2 id="confpapers">Programming Skills</h2>
  <div class="paper">
  <ul>
  <p><font size="5">
      <li>Language: Python, MATLAB, C/C++, LATEX, Markdown and others.</li>
      <li>Machine Learning Frameworks: PyTorch, TensorFlow, Keras, PyG, AllenNLP, Sklearn, OpenCV and others.</li>
  </font></p>
  </ul>
  </div>
  </div>
  </div>
  
  


  <div style="clear: both;">
    <div style="font-size: 10pt;">
    <div class="section">
    <h2 id="confpapers">Links</h2>
    <div class="paper">
    <ul>
    <p><font size="5">
        <li>Collaborators: <a href='http://www1.ece.neu.edu/~yunfu/'>Yun Raymond Fu (NEU)</a>, 
          <a href='http://yulunzhang.com/'>Yulun Zhang (ETH Zürich)</a>, 
          <a href='https://sites.google.com/site/lichenwang123/'>Lichen Wang (Zillow)</a>, 
          <a href='https://scholar.google.com/citations?user=BhysChMAAAAJ&hl=en'>Haoxuan You (Columbia U)</a>,
          <a href='http://huanwang.tech'>Huan Wang (NEU)</a>
          <!-- .</li>
          <li>Site: <a href='https://www.canqin.site'> Can Qin's Blog (TBD) </a>   
            .</li>  --> 
      <!-- <li> Reviewer for CVPR 2019, 2020, ICCV 2019, AAAI 2019, 2020, TIP, TNNLS, TMI, TMM </li>   -->     
      <!-- <li> External Reviewer for CVPR, ICCV, ECCV, AAAI, IJCAI, AAAI (2017). </li> -->
        <!-- <li> Guest reviewer for AAAI 2017. </li>       -->
      <!-- <li> IEEE student member, AAAI student member. </li>    -->
    </font></p>
    </ul>
    </div>
    </div>
    </div>
    </div>
  </div>
</div>


<!-- <div style="clear:both;">
<p align="right"><font size="5">Last Updated on 11th Dec, 2017</a></font></p>
<p align="right"><font size="5">Published with <a href='https://pages.github.com/'>GitHub Pages</a></font></p>
</div> -->

<!-- <hr> -->
<!--  -->
<!-- <div id="clustrmaps-widget"></div><script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=IF-jAGUNTygi5pa59hxIgtJU2XqT-rGoO58Z3E1vHZk&cl=ffffff&w=a"></script> -->
<div align="center">
  <div style="height:200px; width:200px;" > 
    <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=lKYNICJJ1cM7pDCFPKYtMa5bJS9414rBlFiHb7RKULc"></script>
  </div>
  </div>
<!-- -->
</body>
</html>
